{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.koselleck import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_foote(quart=FOOTE_W):\n",
    "    tophalf = [-1] * quart + [1] * quart\n",
    "    bottomhalf = [1] * quart + [-1] * quart\n",
    "    foote = list()\n",
    "    for i in range(quart):\n",
    "        foote.append(tophalf)\n",
    "    for i in range(quart):\n",
    "        foote.append(bottomhalf)\n",
    "    foote = np.array(foote)\n",
    "    return foote\n",
    "\n",
    "def foote_novelty(distdf, foote_size=5):\n",
    "    foote=make_foote(foote_size)\n",
    "    distmat = distdf.values if type(distdf)==pd.DataFrame else distdf\n",
    "    \n",
    "    axis1, axis2 = distmat.shape\n",
    "    assert axis1 == axis2\n",
    "    distsize = axis1\n",
    "    axis1, axis2 = foote.shape\n",
    "    assert axis1 == axis2\n",
    "    halfwidth = axis1 / 2\n",
    "    novelties = []\n",
    "    for i in range(distsize):\n",
    "        start = int(i - halfwidth)\n",
    "        end = int(i + halfwidth)\n",
    "        if start < 0 or end > (distsize - 1):\n",
    "            novelties.append(0)\n",
    "        else:\n",
    "            novelties.append(np.sum(foote * distmat[start: end, start: end]))\n",
    "    return novelties\n",
    "\n",
    "def getyears():\n",
    "    years=list(d.columns)\n",
    "    return years\n",
    "\n",
    "\n",
    "def diagonal_permute(d):\n",
    "    newmat = np.zeros(d.shape)\n",
    "    \n",
    "    # We create one randomly-permuted list of integers called \"translate\"\n",
    "    # that is going to be used for the whole matrix.\n",
    "    \n",
    "    xlen,ylen=d.shape\n",
    "    translate = [i for i in range(xlen)]\n",
    "    random.shuffle(translate)\n",
    "    \n",
    "    # Because distances matrices are symmetrical, we're going to be doing\n",
    "    # two diagonals at once each time. We only need one set of values\n",
    "    # (because symmetrical) but we need two sets of indices in the original\n",
    "    # matrix so we know where to put the values back when we're done permuting\n",
    "    # them.\n",
    "    \n",
    "    for i in range(0, xlen):\n",
    "        indices1 = []\n",
    "        indices2 = []\n",
    "        values = []\n",
    "        for x in range(xlen):\n",
    "            y1 = x + i\n",
    "            y2 = x - i\n",
    "            if y1 >= 0 and y1 < ylen:\n",
    "                values.append(d[x, y1])\n",
    "                indices1.append((x, y1))\n",
    "            if y2 >= 0 and y2 < ylen:\n",
    "                indices2.append((x, y2))\n",
    "        \n",
    "        # Okay, for each diagonal, we permute the values.\n",
    "        # We'll store the permuted values in newvalues.\n",
    "        # We also check to see how many values we have,\n",
    "        # so we can randomly select values if needed.\n",
    "        \n",
    "        newvalues = []\n",
    "        lenvals = len(values)\n",
    "        vallist = [i for i in range(lenvals)]\n",
    "        \n",
    "        for indexes, value in zip(indices1, values):\n",
    "            x, y = indexes\n",
    "            \n",
    "            xposition = translate[x]\n",
    "            yposition = translate[y]\n",
    "            \n",
    "            # We're going to key the randomization to the x, y\n",
    "            # values for each point, insofar as that's possible.\n",
    "            # Doing this will ensure that specific horizontal and\n",
    "            # vertical lines preserve the dependence relations in\n",
    "            # the original matrix.\n",
    "            \n",
    "            # But the way we're doing this is to use the permuted\n",
    "            # x (or y) values to select an index in our list of\n",
    "            # values in the present diagonal, and that's only possible\n",
    "            # if the list is long enough to permit it. So we check:\n",
    "            \n",
    "            if xposition < 0 and yposition < 0:\n",
    "                position = random.choice(vallist)\n",
    "            elif xposition >= lenvals and yposition >= lenvals:\n",
    "                position = random.choice(vallist)\n",
    "            elif xposition < 0:\n",
    "                position = yposition\n",
    "            elif yposition < 0:\n",
    "                position = xposition\n",
    "            elif xposition >= lenvals:\n",
    "                position = yposition\n",
    "            elif yposition >= lenvals:\n",
    "                position = xposition\n",
    "            else:\n",
    "                position = random.choice([xposition, yposition])\n",
    "                # If either x or y could be used as an index, we\n",
    "                # select randomly.\n",
    "            \n",
    "            # Whatever index was chosen, we use it to select a value\n",
    "            # from our diagonal. \n",
    "            \n",
    "            newvalues.append(values[position])\n",
    "            \n",
    "        values = newvalues\n",
    "        \n",
    "        # Now we lay down (both versions of) the diagonal in the\n",
    "        # new matrix.\n",
    "        \n",
    "        for idxtuple1, idxtuple2, value in zip(indices1, indices2, values):\n",
    "            x, y = idxtuple1\n",
    "            newmat[x, y] = value\n",
    "            x, y = idxtuple2\n",
    "            newmat[x, y] = value\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "def zeroless(sequence):\n",
    "    newseq = []\n",
    "    for element in sequence:\n",
    "        if element > 0.01:\n",
    "            newseq.append(element)\n",
    "    return newseq\n",
    "\n",
    "def permute_test(distmatrix, foote_size=FOOTE_W, num_runs=100):\n",
    "    actual_novelties = foote_novelty(distmatrix, foote_size)    \n",
    "    permuted_peaks = []\n",
    "    permuted_troughs = []\n",
    "    xlen,ylen=distmatrix.shape\n",
    "    for i in range(num_runs):\n",
    "        randdist = diagonal_permute(distmatrix)\n",
    "        nov = foote_novelty(randdist, foote_size)\n",
    "        nov = zeroless(nov)\n",
    "        permuted_peaks.append(np.max(nov))\n",
    "        permuted_troughs.append(np.min(nov))\n",
    "    permuted_peaks.sort(reverse = True)\n",
    "    permuted_troughs.sort(reverse = True)\n",
    "    significance_peak = np.ones(len(actual_novelties))\n",
    "    significance_trough = np.ones(len(actual_novelties))\n",
    "    for idx, novelty in enumerate(actual_novelties):\n",
    "        ptop=[i for i,x in enumerate(permuted_peaks) if x and x < novelty]\n",
    "        ptop=ptop[0]/num_runs if ptop else 1\n",
    "        pbot=[i for i,x in enumerate(permuted_troughs) if x and x > novelty]\n",
    "        pbot=pbot[-1]/num_runs if pbot else 1\n",
    "        significance_peak[idx]=ptop\n",
    "        significance_trough[idx]=pbot\n",
    "        \n",
    "        \n",
    "    \n",
    "    return actual_novelties, significance_peak, significance_trough\n",
    "\n",
    "def colored_segments(novelties, significance, yrwidth=1,min_year=1700):\n",
    "    x = []\n",
    "    y = []\n",
    "    t = []\n",
    "    idx = 0\n",
    "    for nov, sig in zip(novelties, significance):\n",
    "        if nov > 1:\n",
    "            x.append((idx*yrwidth) + min_year)\n",
    "            y.append(nov)\n",
    "            t.append(sig)\n",
    "        idx += 1\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    t = np.array(t)\n",
    "    \n",
    "    points = np.array([x,y]).transpose().reshape(-1,1,2)\n",
    "    segs = np.concatenate([points[:-1],points[1:]],axis=1)\n",
    "    lc = LineCollection(segs, cmap=plt.get_cmap('jet'))\n",
    "    lc.set_array(t)\n",
    "    \n",
    "    return lc, x, y\n",
    "    \n",
    "    \n",
    "def test_novelty(distdf, foote_sizes=None, num_runs=100):\n",
    "    if not foote_sizes: foote_sizes=range(FOOTE_W-3, FOOTE_W+2)\n",
    "    dq=distdf.fillna(0).values\n",
    "    o=[]\n",
    "    for fs in foote_sizes:\n",
    "        try:\n",
    "            novelties, significance_peak, significance_trough = permute_test(dq, foote_size=fs, num_runs=num_runs)\n",
    "        except ValueError as e:\n",
    "#             print('!!',e,'!!')\n",
    "#             print(distdf)\n",
    "            continue\n",
    "        for year,nov,sigp,sigt in zip(distdf.columns, novelties, significance_peak, significance_trough):\n",
    "            odx={\n",
    "                'period':year,\n",
    "                'foote_novelty':nov,\n",
    "                'foote_size':fs,\n",
    "                'p_peak':sigp,\n",
    "                'p_trough':sigt,\n",
    "            }\n",
    "            o.append(odx)\n",
    "    return pd.DataFrame(o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_lnm():\n",
    "    with get_veclib('lnm') as vl:\n",
    "        return [x.split(',')[0] for x in vl.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nov_word(word,progress=False,cache=True,force=False,cache_only=False,\n",
    "             interpolate=False,normalize=False,add_missing_periods=True,**kwargs):\n",
    "    odf=None\n",
    "    if cache and not force:\n",
    "        with get_veclib('nov') as vl:\n",
    "            odf=vl.get(word)\n",
    "    \n",
    "    if odf is None or not len(odf):\n",
    "        odf=test_novelty(get_historical_semantic_distance_matrix(\n",
    "                word,\n",
    "                interpolate=interpolate,\n",
    "                normalize=normalize,\n",
    "                progress=progress,\n",
    "                add_missing_periods=add_missing_periods\n",
    "#                 force=force\n",
    "            ),\n",
    "            **kwargs\n",
    "        )\n",
    "        if odf is not None and len(odf):\n",
    "            odf=odf.query('foote_novelty!=0').assign(word=word)\n",
    "        if cache:\n",
    "            with get_veclib('nov',autocommit=True) as vl:\n",
    "                vl[word]=odf\n",
    "    return pd.DataFrame() if (odf is None or cache_only or not len(odf)) else odf.set_index(['word','period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov_word('station',force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(nov_word('ancestor',force=True,interpolate=False).describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(nov_word('ancestor',force=True,interpolate=True).describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(nov_word('ancestor',force=True,interpolate=True,normalize=True).describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(nov_word('ancestor',force=True,interpolate=True,normalize=False).describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(nov_word('ancestor',force=True,interpolate=False,normalize=False).describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(round(nov_word('ancestor',force=True,interpolate=False,normalize=False,add_missing_periods=False).describe(),2))\n",
    "# display(round(nov_word('station',force=True,interpolate=False,normalize=False,add_missing_periods=False).describe(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for w in ['ancestor','station','culture','demand','slave','time']:\n",
    "#     printm('### '+w)\n",
    "#     printm('#### No interpolation')\n",
    "#     display(round(nov_word(w,force=True,interpolate=False,normalize=False,add_missing_periods=False)[['foote_novelty','foote_size']].describe(),2))\n",
    "#     printm('#### Interpolation')\n",
    "#     display(round(nov_word(w,force=True,interpolate=True,normalize=False,add_missing_periods=False)[['foote_novelty','foote_size']].describe(),2))\n",
    "#     printm('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nov_(objd): return nov_word(**objd)\n",
    "\n",
    "def nov(\n",
    "        word_or_words,\n",
    "        progress=True,\n",
    "        cache=True,\n",
    "        force=False,\n",
    "        num_proc=1,\n",
    "        cache_only=False,\n",
    "        ):\n",
    "    words=tokenize_fast(word_or_words) if type(word_or_words)==str else list(word_or_words)\n",
    "    \n",
    "    objs=[\n",
    "        dict(\n",
    "            word=word,\n",
    "            progress=False if len(words)>1 else progress,\n",
    "            cache=cache,\n",
    "            force=force,\n",
    "            cache_only=cache_only,\n",
    "        ) for word in words\n",
    "    ]\n",
    "    o=pmap(\n",
    "        _nov_,\n",
    "        objs,\n",
    "        num_proc=num_proc if len(words)>1 else 1,\n",
    "        progress=progress if len(words)>1 else False,\n",
    "        desc='Measuring novelty across words',\n",
    "    )\n",
    "    return pd.concat(o) if len(o) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov('virtue,vice',force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnov = pd.concat(\n",
    "    grp.assign(foote_novelty_z=(grp.foote_novelty - grp.foote_novelty.mean()) / grp.foote_novelty.std())\n",
    "    for i,grp in tqdm(allnov.groupby('foote_size'))\n",
    "    if i in {4,5,6}\n",
    ")\n",
    "allnov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>foote_novelty</th>\n",
       "      <th>foote_size</th>\n",
       "      <th>p_peak</th>\n",
       "      <th>p_trough</th>\n",
       "      <th>foote_novelty_z</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th>period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">virtue</th>\n",
       "      <th>1740-1745</th>\n",
       "      <td>407.619048</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.156954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745-1750</th>\n",
       "      <td>286.031746</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.203465</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750-1755</th>\n",
       "      <td>289.523810</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.193114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755-1760</th>\n",
       "      <td>190.158730</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.487660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760-1765</th>\n",
       "      <td>311.111111</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.129123</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">indigent</th>\n",
       "      <th>1810-1815</th>\n",
       "      <td>168.750000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.861881</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815-1820</th>\n",
       "      <td>-681.250000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-1.934905</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820-1825</th>\n",
       "      <td>-512.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-1.721878</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825-1830</th>\n",
       "      <td>-456.250000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-1.650869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830-1835</th>\n",
       "      <td>-187.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-1.311604</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517693 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    foote_novelty  foote_size  p_peak  p_trough  \\\n",
       "word     period                                                   \n",
       "virtue   1740-1745     407.619048           4    1.00      1.00   \n",
       "         1745-1750     286.031746           4    1.00      1.00   \n",
       "         1750-1755     289.523810           4    1.00      1.00   \n",
       "         1755-1760     190.158730           4    1.00      0.06   \n",
       "         1760-1765     311.111111           4    1.00      1.00   \n",
       "...                           ...         ...     ...       ...   \n",
       "indigent 1810-1815     168.750000           6    0.98      1.00   \n",
       "         1815-1820    -681.250000           6    1.00      0.99   \n",
       "         1820-1825    -512.500000           6    1.00      0.99   \n",
       "         1825-1830    -456.250000           6    1.00      0.99   \n",
       "         1830-1835    -187.500000           6    1.00      0.99   \n",
       "\n",
       "                    foote_novelty_z  size  \n",
       "word     period                            \n",
       "virtue   1740-1745         0.156954   NaN  \n",
       "         1745-1750        -0.203465   NaN  \n",
       "         1750-1755        -0.193114   NaN  \n",
       "         1755-1760        -0.487660   NaN  \n",
       "         1760-1765        -0.129123   NaN  \n",
       "...                             ...   ...  \n",
       "indigent 1810-1815        -0.861881   NaN  \n",
       "         1815-1820        -1.934905   NaN  \n",
       "         1820-1825        -1.721878   NaN  \n",
       "         1825-1830        -1.650869   NaN  \n",
       "         1830-1835        -1.311604   NaN  \n",
       "\n",
       "[517693 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allnov_f = allnov#.groupby('word').filter(lambda gdf: len(gdf)>=130)\n",
    "allnov_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allnov.loc['virtue'].query('foote_size==6').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnov_w=allnov_f.reset_index().set_index('word').query('foote_size==6')\n",
    "allnov_m=allnov_w.query('p_peak<=0.05').groupby('word')\n",
    "allnov_w['size']=allnov_m.size()\n",
    "allnov_w['size']=allnov_w['size'].fillna(0)\n",
    "# .mean().sort_values('foote_novelty_z',ascending=False)\n",
    "# allnov_m.head(25)\n",
    "# sby=['size','foote_novelty']\n",
    "sby='foote_novelty'\n",
    "allnov_w.groupby('word').mean().sort_values(sby,ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnov.loc['vice'].query('p_peak<0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnov_m=allnov.query('foote_size==5 & p_peak<=0.01').groupby('word').size().sort_values(ascending=False)#.head(25)#'foote_novelty',ascending=False).head(25)\n",
    "# allnov_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_novelty_scores(by_foote_size=False, min_foote_size=6, max_foote_size=6, min_periods=20):\n",
    "    global DFALLNOV\n",
    "    if DFALLNOV is not None:\n",
    "        odf=DFALLNOV\n",
    "    else:\n",
    "        words_done=get_words_with_lnm()\n",
    "        DFALLNOV = odf = nov(words_done,num_proc=4,force=False).query(f'{min_foote_size}<=foote_size<{max_foote_size}')\n",
    "    # set z scores\n",
    "    odf = pd.concat(\n",
    "        grp.assign(\n",
    "            foote_novelty_z=(grp.foote_novelty - grp.foote_novelty.mean()) / grp.foote_novelty.std()\n",
    "        )\n",
    "        for i,grp in odf.groupby('foote_size')\n",
    "    )\n",
    "    \n",
    "    # filter\n",
    "    odf = pd.concat(\n",
    "        grp\n",
    "        for i,grp in allnov[allnov.foote_size==max_foote_size].groupby('word')\n",
    "        if len(grp)>=min_periods\n",
    "    )    \n",
    "    \n",
    "    if not by_foote_size:\n",
    "        odf=odf.groupby(['word','period']).mean().drop('foote_size',1).reset_index()\n",
    "    else:\n",
    "        odf['foote_size']=odf.foote_size.apply(int)\n",
    "        \n",
    "    #odf=odf.query('period<1900')\n",
    "    return odf\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DFALLNOV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3c61c4a847bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_all_novelty_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-b85d713a69ac>\u001b[0m in \u001b[0;36mget_all_novelty_scores\u001b[0;34m(by_foote_size, min_foote_size, max_foote_size, min_periods)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_novelty_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby_foote_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_foote_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_foote_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mDFALLNOV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mDFALLNOV\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0modf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDFALLNOV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DFALLNOV' is not defined"
     ]
    }
   ],
   "source": [
    "get_all_novelty_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_novelty('virtue,value',by_word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_novelty('station', by_word=False).query('foote_novelty!=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nov_all_mean=get_novelty(by_word=False)\n",
    "# nov_all_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_novelty_scores(by_foote_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signif_novelty_scores(p_peak=0.05,min_peaks=1):\n",
    "    odf=get_all_novelty_scores().query(f'p_peak<{p_peak}')\n",
    "    odf=pd.concat(\n",
    "        grp.assign(\n",
    "            word_num_peaks=len(grp[grp.p_peak<p_peak])\n",
    "        ) for i,grp in odf.groupby('word')\n",
    "    )\n",
    "    if min_peaks: odf=odf[odf.word_num_peaks>=min_peaks]\n",
    "    return odf.sort_values('foote_novelty_z',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signif_novelty_scores(p_peak=0.05,min_peaks=1):\n",
    "    odf=get_all_novelty_scores().query(f'p_peak<{p_peak}')\n",
    "    odf=pd.concat(\n",
    "        grp.assign(\n",
    "            word_num_peaks=len(grp[grp.p_peak<p_peak])\n",
    "        ) for i,grp in odf.groupby('word')\n",
    "    )\n",
    "    if min_peaks: odf=odf[odf.word_num_peaks>=min_peaks]\n",
    "    return odf.sort_values('foote_novelty_z',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_signif_novelty_scores(\n",
    "    p_peak=0.05\n",
    ").groupby('word').mean().sort_values('foote_novelty_z',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signif_novelty_words(p_peak=0.05,min_peaks=1):\n",
    "    df=get_all_novelty_scores()\n",
    "    dfsign=get_signif_novelty_scores(p_peak=p_peak,min_peaks=min_peaks)\n",
    "    signwset=set(dfsign.word)\n",
    "    o=[\n",
    "        w for w in \n",
    "        df.groupby('word').mean().sort_values('foote_novelty',ascending=False).index\n",
    "        if w in signwset\n",
    "    ]\n",
    "    print('# all words',len(set(df.word)))\n",
    "    print('# signif words',len(set(dfsign.word)))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_words = get_signif_novelty_words(p_peak=0.05)\n",
    "print(len(sign_words), sign_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all significant words' novelties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_novelty_by_foote_size(p_peak=0.01,min_peaks=1,rolling=2, ymin=-1, nudge_x=1, labsize=6,words={}):\n",
    "    df=get_all_novelty_scores(by_foote_size=True, min_foote_size=4, max_foote_size=6)\n",
    "    if not words: words=get_signif_novelty_words(p_peak=p_peak,min_peaks=min_peaks)\n",
    "#     words={w for w in words if not 's' in w and not 'f' in w}\n",
    "    print('# words used:',len(words))\n",
    "    if words: df=df[df.word.isin(words)]\n",
    "    figdf=pd.DataFrame([\n",
    "        {\n",
    "            'foote_size':fs,\n",
    "            'period':period,\n",
    "            'num_peaks':len(grp.query(f'p_peak<{p_peak}')),\n",
    "            'avg_nov_signif':grp.query(f'p_peak<{p_peak}').foote_novelty_z.mean(),\n",
    "            'avg_nov':grp.foote_novelty_z.mean(),\n",
    "        } for ((fs,period),grp) in df.groupby([\n",
    "            'foote_size','period'\n",
    "        ])\n",
    "    ])\n",
    "    for ycol in ['avg_nov','avg_nov_signif']:\n",
    "        figdf[ycol]=figdf[ycol].rolling(rolling,min_periods=1).mean()\n",
    "    \n",
    "    fig=start_fig(\n",
    "        figdf,\n",
    "        x='period',\n",
    "        y='num_peaks',\n",
    "#         size='num_peaks',\n",
    "        color='factor(foote_size)',\n",
    "#         linetype='factor(foote_size)',\n",
    "    )\n",
    "    fig+=p9.geom_line()\n",
    "    fig+=p9.geom_point(p9.aes(shape='factor(foote_size)'))\n",
    "    \n",
    "    fig+=p9.scale_color_gray(start=.8, end=.2)\n",
    "    fig+=p9.geom_vline(xintercept=1770,linetype='dotted',alpha=0.5) \n",
    "    fig+=p9.geom_vline(xintercept=1800,linetype='dotted',alpha=0.5) \n",
    "    fig+=p9.geom_vline(xintercept=1830,linetype='dotted',alpha=0.5) \n",
    "    fig+=p9.geom_label(label='Sattelzeit begins (1770)',x=1770+nudge_x,y=ymin,angle=90,size=labsize,color='black',va='bottom',boxcolor=(0,0,0,0))\n",
    "    fig+=p9.geom_label(label='Sattelzeit ends (1830)',x=1830+nudge_x,y=ymin,angle=90,size=labsize,color='black',va='bottom',boxcolor=(0,0,0,0)) \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_novelty_by_foote_size(rolling=1, p_peak=.01, min_peaks=1)#, words={'culture'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_novelty_by_foote_size(rolling=1, words={'potato'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfchangepoints=get_signif_novelty_scores(p_peak=.05, min_peaks=1).drop_duplicates('word',keep='first').sort_values('period')\n",
    "dfchangepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odfstr=pd.DataFrame([\n",
    "    {'period':period, 'words':', '.join(grp.sort_values('foote_novelty_z',ascending=False).word)}\n",
    "    for period,grp in sorted(dfchangepoints.groupby('period'))\n",
    "])\n",
    "printm(odfstr.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_novelty_figdf(novdf):\n",
    "    figdf=novdf.sample(frac=1)\n",
    "    ywl=[\n",
    "        f'{x} years'\n",
    "        for x in figdf['foote_size']*5*2\n",
    "    ]\n",
    "    ywls=set(ywl)\n",
    "    ywll=list(reversed(sorted(list(ywls))))\n",
    "    figdf['year_window']=pd.Categorical(ywl, categories=ywll)\n",
    "    figdf['glen']=1\n",
    "    figdf['is_signif']=pd.Categorical(\n",
    "        [bool(x<0.05) for x in figdf.p_peak],\n",
    "        categories=[True,False]\n",
    "    )\n",
    "    \n",
    "    figdf = pd.concat(\n",
    "        grp.assign(foote_novelty_z=grp.foote_novelty.apply(lambda x: (x-grp.foote_novelty.mean())/grp.foote_novelty.std()))\n",
    "        for i,grp in figdf.groupby('foote_size')\n",
    "    )\n",
    "    return figdf.dropna().sort_values(['year_window','period'])\n",
    "\n",
    "\n",
    "# @interact\n",
    "def plot_novelty(\n",
    "        words=None,\n",
    "        novdf=None,\n",
    "        color='factor(year_window)',\n",
    "        group='factor(year_window)',\n",
    "        shape='factor(year_window)',\n",
    "        size='glen',\n",
    "        max_p_peak=None,\n",
    "        vnum='v9',\n",
    "        showdata=False,\n",
    "        xlab='Date of semantic model',\n",
    "        ylab='Foote Novelty (standardized)',\n",
    "        colorlab='Foote matrix width',\n",
    "        shapelab='Foote matrix width',\n",
    "        sizelab='Number of significant peaks',\n",
    "        title='Average novelty score for significant words over time',\n",
    "        rolling=2,\n",
    "        min_periods=1,\n",
    "        min_foote_size=6,\n",
    "        max_foote_size=6,\n",
    "        y='foote_novelty',\n",
    "        ymin=-.1,\n",
    "        ylim0=0,\n",
    "        ylim1=20,\n",
    "        use_ylim=False,\n",
    "        xlim0=1750,\n",
    "        xlim1=1900,\n",
    "        sizemin=.25,\n",
    "        sizemax=2,\n",
    "        labsize=6,\n",
    "        hline='',\n",
    "        nudge_label_y=1,\n",
    "        ymin_heatmap=1750,\n",
    "        combine=False,\n",
    "        use_color=False,\n",
    "        h_fig1=4.00,\n",
    "        h_fig2=4.00,\n",
    "        nudge_x=3,\n",
    "        xlab_min=1735,\n",
    "        add_median=True,\n",
    "        save=False,\n",
    "        label_words=False,\n",
    "        logy=False,\n",
    "        show_period_labels=True,\n",
    "        dist_invert_fill=False,\n",
    "        line_size=0.5,\n",
    "        label_size=7,\n",
    "        by_word=False\n",
    "        ):\n",
    "\n",
    "    figwords=set(words) if words else {'allwords'}\n",
    "    if novdf is None:\n",
    "        if words is None:\n",
    "            print('neither words nor novdf')\n",
    "            return\n",
    "        \n",
    "        novdf = get_novelty(words,by_word=by_word)\n",
    "        if not by_word: words=None\n",
    "        print(f'Computed novelty df of shape {novdf.shape}')\n",
    "#         display(novdf)\n",
    "        \n",
    "#     figdf=get_plot_novelty_figdf(novdf.query(f'{min_foote_size}<=foote_size<={max_foote_size}'))\n",
    "    figdf=get_plot_novelty_figdf(novdf)\n",
    "    if not len(figdf): return\n",
    "    if max_p_peak: figdf=figdf[figdf.p_peak<max_p_peak]\n",
    "    \n",
    "    \n",
    "    figdf=figdf.sort_values('period')\n",
    "    if showdata: display(figdf)\n",
    "    fig=start_fig(\n",
    "        figdf,\n",
    "        x='period',\n",
    "        y=y,\n",
    "        color=color if color else None,\n",
    "        group=group if group else None,\n",
    "        figure_size=(8,h_fig1)\n",
    "    )\n",
    "    \n",
    "    if add_median:\n",
    "        kname='Guides'\n",
    "        mediandf=pd.DataFrame([{\n",
    "            'yintercept':figdf[y].median(),\n",
    "            kname:'Median',\n",
    "        },\n",
    "        ])\n",
    "        fig+=p9.geom_hline(\n",
    "            p9.aes(yintercept='yintercept',linetype=kname),\n",
    "            data=mediandf,\n",
    "            size=.25,\n",
    "            show_legend=True\n",
    "        )\n",
    "    fig+=p9.geom_line(size=line_size)\n",
    "    pntd={}\n",
    "    if size: pntd['size']=size\n",
    "    if shape: pntd['shape']=shape\n",
    "    fig+=p9.geom_point(p9.aes(**pntd))\n",
    "    fig+=p9.labs(x=xlab,y=ylab,title=title,color=colorlab,size=sizelab,shape=shapelab)\n",
    "    if use_ylim: fig+=p9.ylim(ylim0,ylim1)\n",
    "    fig+=p9.scale_size_continuous(range=(sizemin,sizemax))\n",
    "    if not use_color: fig+=p9.scale_color_gray(direction=1)# if not use_color else p9.scale_color_distiller(type='qual')\n",
    "    if hline not in {None,''}:\n",
    "        fig+=p9.geom_hline(yintercept=hline,linetype='dotted')\n",
    "    if words and label_words:\n",
    "        labeldf=figdf[figdf.is_signif==1]\n",
    "        grps=[\n",
    "            grp.sort_values(y).iloc[-1:]\n",
    "            for i,grp in labeldf.groupby('word')\n",
    "        ]\n",
    "        if len(grps):\n",
    "            labeldf=pd.concat(grps)\n",
    "            labeldf[y]+=nudge_label_y\n",
    "            fig+=p9.geom_label(p9.aes(label='word'),color='black',\n",
    "                               size=label_size,data=labeldf,boxcolor=(0,0,0,0))\n",
    "    if show_period_labels:\n",
    "        fig+=p9.geom_vline(xintercept=1770,linetype='dotted',alpha=0.5) \n",
    "        fig+=p9.geom_vline(xintercept=1800,linetype='dotted',alpha=0.5) \n",
    "        fig+=p9.geom_vline(xintercept=1830,linetype='dotted',alpha=0.5) \n",
    "        fig+=p9.geom_label(label='Sattelzeit begins (1770)',x=1770+nudge_x,y=ymin,angle=90,size=labsize,color='black',va='bottom',boxcolor=(0,0,0,0))\n",
    "        fig+=p9.geom_label(label='Sattelzeit ends (1830)',x=1830+nudge_x,y=ymin,angle=90,size=labsize,color='black',va='bottom',boxcolor=(0,0,0,0)) \n",
    "    if size=='is_signif':\n",
    "        fig+=p9.scale_size_manual({True:2,False:.2})\n",
    "    else:\n",
    "        fig+=p9.scale_size_continuous(range=[.25,3])\n",
    "    fig+=p9.theme_minimal()\n",
    "    fig+=p9.theme(axis_text_x=p9.element_text(angle=90), text=p9.element_text(size=8))\n",
    "    if logy: fig+=p9.scale_y_log10(limits=[ylim0,ylim1])\n",
    "    fig+=p9.scale_x_continuous(\n",
    "        minor_breaks=list(range(xlim0//5*5,(xlim1//5*5)+5,5)),\n",
    "        limits=[xlim0,xlim1]\n",
    "    )\n",
    "    wkey=''\n",
    "    if words: wkey=words.replace(' ','') if type(words)==str else '-'.join(words)\n",
    "    ofn=f'''fig.footenov.{vnum}.{wkey+'.' if wkey else ''}{'cmbo.' if combine else ''}png'''\n",
    "    ofnfn=os.path.join(PATH_FIGS,ofn)\n",
    "\n",
    "    if combine:\n",
    "        yymin1=figdf.period.min()\n",
    "        yymax1=figdf.period.max()\n",
    "        figdm=plot_historical_semantic_distance_matrix(words=figwords,ymin=xlim0,ymax=xlim1)\n",
    "        ofig=combine_plots(figdm,fig,ofn=ofnfn)\n",
    "    else:\n",
    "        ofig=fig\n",
    "        if save: ofig.save(ofnfn)\n",
    "    display(ofig)\n",
    "    if save: upfig(ofnfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_novelty_words(words,**kwargs):\n",
    "    words=[w.strip() for w in words.split(',')] if type(words)==str else list(words)\n",
    "    inpd=dict(\n",
    "        y='foote_novelty_z',\n",
    "        words=words,\n",
    "        color='word',\n",
    "        group='word',\n",
    "        shape='word',\n",
    "        colorlab='Word',\n",
    "        shapelab='Word',\n",
    "        sizelab='Statistically significant',\n",
    "        title='Novelty scores for key words',\n",
    "        ylab='Foote Novelty score',\n",
    "        size='is_signif',\n",
    "        vnum='v19',\n",
    "        use_ylim=False,\n",
    "        add_median=True,\n",
    "        max_p_peak=0.0,\n",
    "        min_foote_size=5,\n",
    "        max_foote_size=5,\n",
    "        showdata=False,\n",
    "        nudge_x=2,\n",
    "        logy=False,\n",
    "        ylim0=0,\n",
    "        ylim1=10,\n",
    "        xlim0=1740,\n",
    "        xlim1=1940,\n",
    "        rolling=2,\n",
    "        ymin=.1,\n",
    "        label_words=True,\n",
    "        show_period_labels=True,\n",
    "        nudge_label_y=0.25,\n",
    "        save=True,\n",
    "        by_word=True\n",
    "    )\n",
    "    return plot_novelty(**{**inpd, **kwargs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_novelty_words('station,value,commerce,growth,culture,slave,slavery,god,time,december')\n",
    "plot_novelty_words('station,value,slave,demand,interest,circulation,improvement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
