{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.koselleck import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_veclib_word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated elsewhere..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgrams(idir=PATH_SKIPGRAMS_YR,skipgram_n=25, calc_numlines=False):\n",
    "    odf=pd.DataFrame([\n",
    "        {\n",
    "            'corpus':fn.split('.')[2],\n",
    "            'year':int([x for x in fn.split('.') if x.isdigit()][0]),\n",
    "#             'period_end':int([x for x in fn.split('.') if x.isdigit()][-1]),\n",
    "            'path':os.path.join(idir,fn)\n",
    "        }\n",
    "        for fn in os.listdir(idir)\n",
    "        if fn.startswith('data.skipgrams')\n",
    "    ]).sort_values(['corpus','year'])\n",
    "    if calc_numlines:\n",
    "        odf['num_lines']=odf.path.progress_apply(get_numlines)\n",
    "        odf['num_words']=odf['num_lines']*skipgram_n\n",
    "    return odf#.query('1680<=year<1970')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_skipgrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfskip=get_skipgrams(calc_numlines=True)\n",
    "# dfskip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dfskip['period']=dfskip.year.apply(lambda y: periodize_bystep(y,8,1700))\n",
    "# dfskip['period']=dfskip.year.apply(lambda y: periodize_bystep(y,8,1700))\n",
    "# dfskip.groupby('period').sum().applymap(lambda x: f'{x:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfskipruns(dfskip,num_runs=10,incl_existing=False):\n",
    "    dfskipruns=pd.concat([\n",
    "        dfskip.assign(run=f'run_{str(i+1).zfill(2)}')\n",
    "        for i in range(num_runs)\n",
    "    ])\n",
    "    dfskipruns['opath']=dfskipruns.apply(lambda row: os.path.join(PATH_MODELS,row.corpus,row.period,row.run,'model.bin'),1)\n",
    "    dfskipruns['opath_exists']=dfskipruns.opath.apply(lambda x: os.path.exists(x))\n",
    "    if not incl_existing: dfskipruns=dfskipruns[dfskipruns.opath_exists==False]\n",
    "    return dfskipruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dfskipruns(dfskip,num_runs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkipgramsSampler2:\n",
    "    def __init__(self,fn,num_skips_wanted=100000,replace=True):\n",
    "        self.fn=fn\n",
    "        self.num_skips_wanted=num_skips_wanted\n",
    "        if os.path.exists(fn):\n",
    "            with xopen(fn) as f:\n",
    "                lines=[ln.strip().split() for ln in f if ln.strip()]\n",
    "        if not replace and len(lines)<num_skips_wanted:\n",
    "            num_skips_wanted=len(lines)\n",
    "        self.skips=random.choices(lines,k=num_skips_wanted)\n",
    "    def __iter__(self):\n",
    "        yield from self.skips\n",
    "    def __len__(self): return len(self.skips)\n",
    "\n",
    "\n",
    "\n",
    "class SkipgramsSampler:\n",
    "    def __init__(self, fn, num_skips_wanted=None, replace=False):\n",
    "            self.fn=fn\n",
    "            self.replace=replace\n",
    "            \n",
    "            if num_skips_wanted:                \n",
    "                self.num_skips=self.get_num_lines()\n",
    "                self.num_skips_wanted=nskip=num_skips_wanted if self.num_skips>num_skips_wanted else self.num_skips\n",
    "                self.line_nums_wanted = set(random.sample(list(range(nskip)), nskip))\n",
    "            else:\n",
    "                self.num_skips_wanted=None\n",
    "\n",
    "    def get_num_lines(self):\n",
    "            then=time.time()\n",
    "#               print('>> [SkipgramsSampler] counting lines in',self.fn)\n",
    "            with gzip.open(self.fn,'rb') if self.fn.endswith('.gz') else open(self.fn) as f:\n",
    "                    for i,line in enumerate(f):\n",
    "                            pass\n",
    "            num_lines=i+1\n",
    "            now=time.time()\n",
    "#               print('>> [SkipgramsSampler] finished counting lines in',self.fn,'in',int(now-then),'seconds. # lines =',num_lines,'and num skips wanted =',self.num_skips_wanted)\n",
    "            return num_lines\n",
    "\n",
    "    def __iter__(self):\n",
    "            i=0\n",
    "            with gzip.open(self.fn,'rb') if self.fn.endswith('.gz') else open(self.fn) as f:\n",
    "                    for i,line in enumerate(f):\n",
    "                            line = line.decode('utf-8') if self.fn.endswith('.gz') else line\n",
    "                            if not self.num_skips_wanted or i in self.line_nums_wanted:\n",
    "                                    yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramsSamplers:\n",
    "    def __init__(self,fns,nskip,sampler=SkipgramsSampler,**y):\n",
    "        self.skippers=[sampler(fn,nskip) for fn in fns]\n",
    "    def __iter__(self):\n",
    "        for skipper in self.skippers:\n",
    "            yield from skipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipfn=dfskip.iloc[0].path\n",
    "# iterr=SkipgramsSampler(skipfn,None)\n",
    "# l=[]\n",
    "# for x in tqdm(iterr): l+=[x]\n",
    "# print(random.choice(l))\n",
    "# print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_SKIP=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_save_model(dfskip,nskip=DEFAULT_NUM_SKIP,force=False,vector_size=100,window=10,min_count=5,epochs=10,workers=8,verbose=False):\n",
    "    row=dfskip.iloc[0]\n",
    "    odir=os.path.join(PATH_MODELS_NEW,row.corpus,row.period,row.run)\n",
    "    ofnfn=os.path.join(odir,'model.bin')\n",
    "    if force or not os.path.exists(ofnfn):\n",
    "        ensure_dir_exists(odir)\n",
    "        ss=SkipgramsSamplers(dfskip.path, nskip)\n",
    "        disable_gensim_logging() if not verbose else enable_gensim_logging()\n",
    "        model = Word2Vec(sentences=ss,vector_size=vector_size,window=window,min_count=min_count,epochs=epochs,workers=workers)\n",
    "        model.save(ofnfn)\n",
    "    return pd.DataFrame([{'fnfn':ofnfn}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnfn=gen_and_save_model(get_dfskipruns(dfskip).iloc[:1], force=True).fnfn.iloc[0]\n",
    "# load_model(fnfn).wv.most_similar('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res=pmap_groups(\n",
    "#     gen_and_save_model,\n",
    "#     dfskipruns.groupby(['period','run']),\n",
    "#     num_proc=4,\n",
    "#     kwargs=dict(force=True, nskip=NSKIP_PER_YR)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodize(1801,40,1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_models(\n",
    "        ybin=5,\n",
    "        ymin=1680,\n",
    "        ymax=1970,\n",
    "        num_runs=1,\n",
    "        force=False,\n",
    "        nskip_per_yr=NSKIP_PER_YR,\n",
    "        verbose=False,\n",
    "        num_proc=1\n",
    "    ):\n",
    "    dfskip=get_skipgrams(calc_numlines=False).query(f'{ymin}<=year<{ymax}')\n",
    "    dfskip['period']=dfskip.year.apply(lambda y: periodize_bystep(y,ybin,ymin))\n",
    "    dfskip=dfskip[dfskip.period!='']\n",
    "    dfskipruns=get_dfskipruns(dfskip, num_runs=num_runs, incl_existing=force)\n",
    "    dfgrps=dfskipruns.groupby(['period','run'])\n",
    "    print(f'Generating {len(dfgrps)} new models over {dfskipruns.period.nunique()} periods and {dfskipruns.run.nunique()} runs')\n",
    "    return pmap_groups(\n",
    "        gen_and_save_model,\n",
    "        dfskipruns.groupby(['period','run']),\n",
    "        num_proc=num_proc,\n",
    "        kwargs=dict(force=force, nskip=nskip_per_yr,verbose=verbose),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Starting')\n",
    "# # gen_models(ybin=40,ymin=1700,ymax=1900,num_runs=10,nskip_per_yr=None)\n",
    "# # gen_models(ybin=8,ymin=1700,ymax=1900,num_runs=1,nskip_per_yr=None,verbose=False,num_proc=2)\n",
    "# # gen_models(ybin=8,ymin=1700,ymax=1900,num_runs=10,nskip_per_yr=None,verbose=False,num_proc=2)\n",
    "# gen_models(ybin=20,ymin=1700,ymax=1900,num_runs=10,nskip_per_yr=None,num_proc=3,verbose=False)\n",
    "# print('Done')\n",
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_models(num_runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def get_model_paths(model_dir=PATH_MODELS,model_fn='model.bin',vocab_fn='vocab.txt',period_len=None):\n",
    "    \"\"\"\n",
    "    Get all models' paths\n",
    "    \"\"\"\n",
    "    ld=[]\n",
    "    for root,dirs,fns in tqdm(os.walk(model_dir),desc='Scanning directory for models'):\n",
    "        if model_fn in fns:\n",
    "            corpus,period,run=root.split('/')[-3:]\n",
    "            if not 'run_' in run:\n",
    "                corpus,period=root.split('/')[-2:]\n",
    "                run=None\n",
    "            dx={\n",
    "                'corpus':corpus,\n",
    "                'period_start':int(period.split('-')[0]),\n",
    "                'period_end':int(period.split('-')[-1]),\n",
    "                'path':os.path.join(root,model_fn),\n",
    "                'path_vocab':os.path.join(root,vocab_fn)\n",
    "            }\n",
    "            if run is not None: dx['run']=run\n",
    "            if period_len and int(dx['period_end'])-int(dx['period_start'])!=period_len:\n",
    "                continue\n",
    "            ld.append(dx)\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 1960, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YMIN,YMAX,YEARBIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathdf_models(period_len=YEARBIN,ymin=YMIN,ymax=YMAX):\n",
    "    pathdf=pd.DataFrame(get_model_paths(PATH_MODELS_BPO, 'model.bin'))#.sort_values(['period_start','run'])\n",
    "    pathdf['period']=[f'{x}-{y}' for x,y in zip(pathdf.period_start, pathdf.period_end)]\n",
    "    pathdf['period_len']=pathdf.period_end - pathdf.period_start\n",
    "    pathdf['qstr']=[\n",
    "        f'vecs({period}_{run.split(\"_\")[-1]})'\n",
    "        for period,run in zip(pathdf.period, pathdf.run)\n",
    "    ]\n",
    "    if period_len: pathdf=pathdf[pathdf.period_len==period_len]\n",
    "    if ymin: pathdf=pathdf[pathdf.period_start>=ymin]\n",
    "    if ymax: pathdf=pathdf[pathdf.period_end<=ymax]\n",
    "    return pathdf[~pathdf.period.isnull()].sort_values('period_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pathdf_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_models(ymin=YMIN,ymax=YMAX,ybin=YEARBIN,num_runs=10):\n",
    "    if os.path.exists(FN_DEFAULT_MODEL_PATHS):\n",
    "        odf=read_df(FN_DEFAULT_MODEL_PATHS)\n",
    "    else:\n",
    "        odf=get_pathdf_models(period_len=ybin)\n",
    "        odf.to_pickle(FN_DEFAULT_MODEL_PATHS)\n",
    "    return odf.query(f'{ymin}<=period_start & period_end<={ymax} & run<=\"run_{num_runs:02}\"')\n",
    "    \n",
    "\n",
    "def get_default_periods(**y):\n",
    "    return sorted(list(set(get_default_models(**y).period)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD_LENS={5,20,70}\n",
    "\n",
    "def get_dfmodels(ybins=PERIOD_LENS,ymin=YMIN,ymax=YMAX):\n",
    "    dfmodels = get_pathdf_models(period_len=None,ymax=ymax,ymin=ymin)\n",
    "    dfmodels = dfmodels[dfmodels.period_len.isin(PERIOD_LENS)]\n",
    "    dfmodels['run_int']=dfmodels['run'].apply(lambda x: int(x.split('_')[-1]))\n",
    "    return dfmodels.sort_values(['period_start','run_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dfmodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_periods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_periods_runs(period_or_periods=None,run_or_runs=None,num_runs=10):\n",
    "    periods=period_or_periods\n",
    "    if periods is None: periods=get_default_periods()\n",
    "    if type(periods)==str: periods=tokenize_fast(periods)\n",
    "    periods=set(periods)\n",
    "    runs=run_or_runs    \n",
    "    if runs is None: runs=list(range(1,num_runs+1))\n",
    "    if type(runs)==int: runs=[runs]\n",
    "    if type(runs)==str: runs=[int(runs)]\n",
    "    runs=set(runs)\n",
    "    return periods,runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_periods_runs('1770-1775,1780-1785')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_periods(ymin=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(path_model,path_vocab=None,min_count=None,cache_bin=True,cache=False):\n",
    "    global MODEL_CACHE\n",
    "    \n",
    "    if cache and path_model in MODEL_CACHE: return MODEL_CACHE[path_model]\n",
    "#     print('Loading',path_model)\n",
    "    model=do_load_model(path_model,path_vocab=path_vocab,min_count=min_count,cache_bin=cache_bin)\n",
    "    return model\n",
    "    \n",
    "def do_load_model(path_model,path_vocab=None,min_count=None,cache_bin=True):\n",
    "#     print('>> loading',path_model)\n",
    "    path_model_bin=path_model.split('.txt')[0]+'.bin' if not path_model.endswith('.bin') else path_model\n",
    "    if os.path.exists(path_model_bin):\n",
    "        model=gensim.models.KeyedVectors.load(path_model_bin,mmap='r')\n",
    "    elif os.path.exists(path_model):\n",
    "        if not path_vocab: path_vocab=os.path.join(os.path.dirname(path_model,'vocab.txt'))\n",
    "        if os.path.exists(path_vocab):\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path_model,path_vocab)\n",
    "            if min_count: filter_model(model,min_count=min_count)\n",
    "        else:\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path_model)\n",
    "        if cache_bin:\n",
    "            model.save(path_model_bin)\n",
    "    else:\n",
    "        return None\n",
    "#     print(path_model, len(model.wv.key_to_index))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_row(row): return load_model(row.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = get_default_models().sample(n=1).iloc[0]\n",
    "# load_model_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=load_model('/home/ryan/github/koselleck/data1/models/bpo/1805-1810/run_25/model.bin')\n",
    "# m.wv.most_similar('virtue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=load_model('/home/ryan/github/koselleck/data1/models/bpo/1945-1950/run_07/model.bin')\n",
    "# m.wv.most_similar(['king','woman'],['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(dfmodels,gby=['period','run']):\n",
    "    o=[]\n",
    "    dfgrp=dfmodels.groupby(gby)\n",
    "    for period,dfg in tqdm(sorted(dfgrp)):#, total=len(dfgrp)):\n",
    "        path=dfg.iloc[-1].path\n",
    "        m=load_model(path)\n",
    "        try:\n",
    "            testvec=m.wv.most_similar(['king','woman'],['man'],topn=25)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        testvec_wl=[x for x,y in testvec]\n",
    "        has_queen='queen' in set(testvec_wl)\n",
    "        odx={\n",
    "            **dict(zip(gby,period)),\n",
    "            'has_queen':has_queen,\n",
    "            'rank_queen':testvec_wl.index('queen') if has_queen else np.nan,\n",
    "            'neighborhood':', '.join(testvec_wl),\n",
    "        }\n",
    "        o+=[odx]\n",
    "#         break\n",
    "    return pd.DataFrame(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmodels = get_pathdf_models().query('period_len==5')\n",
    "# dftests  = test_models(dfmodels)\n",
    "# dftests.to_csv('../../data/data.model.tests.csv')\n",
    "# dftests.query('has_queen==True').groupby('period').size()\n",
    "# dftests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_veclib_word_data_path(word):\n",
    "    ofn=os.path.join(PATH_DB,'cdists',f'data.cdists.{word}.pkl.gz')\n",
    "    odir=os.path.dirname(ofn)\n",
    "    if not os.path.exists(odir): os.makedirs(odir)\n",
    "    return ofn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_new_veclib_word_data_path('virtue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_veclib_word_data(word,progress=True,cache=True,cache_only=False,force=False,remove_old=False,\n",
    "                        periods=None):\n",
    "    if progress: print(f'Loading cdist data for \"{word}\"')\n",
    "    odf=pd.DataFrame()\n",
    "    fnfn=get_new_veclib_word_data_path(word)\n",
    "    oldfnfn=get_old_veclib_word_data_path(word)\n",
    "    if cache and not force and os.path.exists(fnfn):\n",
    "        try:\n",
    "            odf=read_df(fnfn)\n",
    "            if progress: print(f'Finished loading cdist data from pkl for \"{word}\"')\n",
    "        except Exception as e:\n",
    "            print('!!',e)\n",
    "    if not len(odf):\n",
    "        if not os.path.exists(oldfnfn):\n",
    "            if progress: print(f'No file found at {oldfnfn}')\n",
    "        else:\n",
    "            with get_veclib_word(word) as vl:\n",
    "                dfdist=pd.DataFrame(dict(vl.items())).T.rename_axis('period_run_')\n",
    "                dfdist['period_'],dfdist['run_']=zip(*[x.split('_') for x in dfdist.index])\n",
    "                dfdist['run_']=dfdist['run_'].apply(int)\n",
    "                odf=dfdist.reset_index().drop('period_run_',1).set_index(['period_','run_'])\n",
    "                if cache:\n",
    "                    if progress: print(f'Saving dfdist to \"{fnfn}\"')\n",
    "                    odf.to_pickle(fnfn)\n",
    "                    if remove_old and os.path.exists(oldfnfn):\n",
    "                        if progress: print(f'Removing old data from \"{oldfnfn}\"')\n",
    "                        os.remove(oldfnfn)\n",
    "                if progress: print(f'Finished loading cdist data from sqlite for \"{word}\"')\n",
    "    if not len(odf): return odf\n",
    "    odf['word_']=word\n",
    "    odf=odf.reset_index()\n",
    "    \n",
    "    if periods is None: periods=set(get_default_periods())\n",
    "    odf=odf[odf.period_.isin(periods)]\n",
    "    odf=odf.set_index(['word_','period_','run_']).rename_axis(['word','period','run'])\n",
    "    return odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfdist=get_veclib_word_data('histories',force=False,remove_old=False,cache_only=False)\n",
    "# dfdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_in_sqlite_data():\n",
    "    fns=os.listdir(os.path.join(PATH_DB,'wvecs'))\n",
    "    words=[fn.split('.sqlite')[0].split('.')[-1] for fn in fns]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=get_all_words_in_sqlite_data()\n",
    "# len(words),random.sample(words,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_veclib_word_data_(objd): return get_veclib_word_data(**objd)\n",
    "def reformat_all_sqlite_data(words=None,lim=None,num_proc=1,remove_old=True):\n",
    "    words=get_all_words_in_sqlite_data()[:lim] if words is None else list(words)[:lim]\n",
    "    return pmap(\n",
    "        _get_veclib_word_data_,\n",
    "        [dict(word=word,progress=False,cache=True,force=True,remove_old=remove_old,\n",
    "              cache_only=True) for word in words],\n",
    "        num_proc=num_proc,\n",
    "        desc='Reformatting old sqlite data into pkl files',\n",
    "        use_threads=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=get_valid_words()\n",
    "# random.shuffle(words)\n",
    "# res=reformat_all_sqlite_data(words,lim=None,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def measure_ambiguity(model,words=None,topn=10):\n",
    "    dfdist=to_dist(model,words=words)\n",
    "    g=to_semnet_from_dist(dfdist,topn=topn)\n",
    "    s=pd.Series(nx.clustering(g)).sort_values()\n",
    "    amb=1-s\n",
    "    return amb\n",
    "\n",
    "def to_dist(m,words=None,z=1,norm=1,maxwords=10000):\n",
    "    if words is None:\n",
    "        words=[m.wv.index_to_key[i] for i in range(maxwords)]\n",
    "    else:\n",
    "        words=set(words) & set(m.wv.key_to_index.keys())\n",
    "    words=list(words)\n",
    "    vecs = np.array([m.wv[w] for w in words], dtype=np.float64)\n",
    "    omatrix = fastdist.cosine_pairwise_distance(vecs, return_matrix=True)\n",
    "    odf = round(pd.DataFrame(omatrix, index=words, columns=words),6)\n",
    "    maxv=odf.max().max()\n",
    "    odf=odf.replace({maxv:np.nan})\n",
    "    if norm: odf=maxv - odf\n",
    "    if z: odf=(odf - odf.mean().mean())/odf.std().std()\n",
    "    return odf\n",
    "\n",
    "def to_semnet_from_dist(dfdist,cutoff=3,topn=10):\n",
    "    g=nx.Graph()\n",
    "    for word1 in dfdist.columns:\n",
    "        for word2,val in dfdist[word1].sort_values(ascending=True).iloc[:topn].items():\n",
    "            g.add_edge(word1,word2,weight=val*-1)\n",
    "    return g\n",
    "\n",
    "def measure_ambiguity(model,words=None,topn=10,z=False):\n",
    "    dfdist=to_dist(model,words=words)\n",
    "    g=to_semnet_from_dist(dfdist,topn=topn)\n",
    "    s=pd.Series(nx.clustering(g)).sort_values()\n",
    "    amb=1-s\n",
    "    if z: amb=(amb - amb.mean())/amb.std()\n",
    "    return amb\n",
    "\n",
    "def get_any_model(dfpath=None):\n",
    "    if dfpath is None: dfpath=get_pathdf_models_bydecade()\n",
    "    row=dfpath.sample(n=1).iloc[0]\n",
    "    return load_model_row(row)\n",
    "\n",
    "def measure_freq(model,words=None,tf=True,z=False):\n",
    "    mwords=set(model.wv.key_to_index.keys())\n",
    "    words=mwords if not words else mwords&set(words)\n",
    "    vocabd=dict(\n",
    "        (\n",
    "            w,\n",
    "            model.wv.get_vecattr(w,'count')\n",
    "        )\n",
    "        for w in words\n",
    "    )\n",
    "    svocab=pd.Series(vocabd)\n",
    "    if tf: svocab=svocab / svocab.sum()\n",
    "    if z: svocab=(svocab - svocab.mean()) / svocab.std()\n",
    "    return svocab\n",
    "\n",
    "INFLECTER=None\n",
    "def measure_singularism(m,words=None,z=True):\n",
    "    global INFLECTER\n",
    "    if INFLECTER is None:\n",
    "        import inflect\n",
    "        INFLECTER=inflect.engine() \n",
    "    p=INFLECTER\n",
    "\n",
    "    if not words: words=get_valid_words()\n",
    "    words=list(set(words) & set(m.wv.key_to_index.keys()))\n",
    "    words_plurals = pmap(p.plural_noun, words, num_proc=1, progress=False)\n",
    "    s=measure_freq(m,words=set(words+words_plurals),z=z)\n",
    "    sd=dict(s)\n",
    "    odf=pd.DataFrame([\n",
    "        {'word':ws, 'word_pural':wp, 'freq_sing':sd.get(ws), 'freq_plural':sd.get(wp)}\n",
    "        for ws,wp in zip(words,words_plurals)\n",
    "        if ws!=wp\n",
    "    ])\n",
    "    odf['freq_diff']=odf['freq_sing']-odf['freq_plural']\n",
    "    odf['rank_sing']=odf['freq_sing'].rank(ascending=True)\n",
    "    odf['rank_plural']=odf['freq_plural'].rank(ascending=True)\n",
    "    odf['rank_diff']=odf['rank_sing']-odf['rank_plural']\n",
    "    if z:\n",
    "        for x in odf.select_dtypes('number').columns:\n",
    "            odf[x]=(odf[x] - odf[x].mean())/odf[x].std()\n",
    "    return odf.set_index('word').sort_values('rank_diff').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None, combine_models=False):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    if combine_models:\n",
    "        o=union_align_gensim(base_embed,other_embed)\n",
    "    else:\n",
    "        o=other_embed\n",
    "    return o\n",
    "    \n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "    return (m1,m2)\n",
    "\n",
    "\n",
    "def union_align_gensim(m1, m2, words=None, suffix1='_m1',suffix2='_m2'):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    new_arr_both=[]\n",
    "    common_vocab_both=[w+suffix1 for w in common_vocab] + [w+suffix2 for w in common_vocab]\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = [old_arr[index] for index in indices]\n",
    "        new_arr_both+=new_arr\n",
    "    \n",
    "#     odf=pd.DataFrame(new_arr_both,index=common_vocab_both)\n",
    "#     return odf\n",
    "    \n",
    "    m.wv.vectors = np.array(new_arr_both)\n",
    "\n",
    "    # Replace old vocab dictionary with new one (with common vocab)\n",
    "    # and old index2word with new one\n",
    "    new_key_to_index = {}\n",
    "    new_index_to_key = []\n",
    "    for new_index, key in enumerate(common_vocab_both):\n",
    "        new_key_to_index[key] = new_index\n",
    "        new_index_to_key.append(key)\n",
    "    m.wv.key_to_index = new_key_to_index\n",
    "    m.wv.index_to_key = new_index_to_key\n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
