{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.koselleck import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_veclib_word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated elsewhere..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgrams(idir=PATH_SKIPGRAMS_YR,skipgram_n=25, calc_numlines=False):\n",
    "    odf=pd.DataFrame([\n",
    "        {\n",
    "            'corpus':fn.split('.')[2],\n",
    "            'year':int([x for x in fn.split('.') if x.isdigit()][0]),\n",
    "#             'period_end':int([x for x in fn.split('.') if x.isdigit()][-1]),\n",
    "            'path':os.path.join(idir,fn)\n",
    "        }\n",
    "        for fn in os.listdir(idir)\n",
    "        if fn.startswith('data.skipgrams')\n",
    "    ]).sort_values(['corpus','year'])\n",
    "    if calc_numlines:\n",
    "        odf['num_lines']=odf.path.progress_apply(get_numlines)\n",
    "        odf['num_words']=odf['num_lines']*skipgram_n\n",
    "    return odf#.query('1680<=year<1970')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_skipgrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>year</th>\n",
       "      <th>path</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1681</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1681.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1682</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1682.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1683</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1683.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1684</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1684.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1685</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1685.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1967</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1967.txt</td>\n",
       "      <td>1940-1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1968</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1968.txt</td>\n",
       "      <td>1940-1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1969</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1969.txt</td>\n",
       "      <td>1940-1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1970</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1970.txt</td>\n",
       "      <td>1940-1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1974</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1974.txt</td>\n",
       "      <td>1940-1980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus  year  \\\n",
       "182    bpo  1681   \n",
       "56     bpo  1682   \n",
       "51     bpo  1683   \n",
       "60     bpo  1684   \n",
       "200    bpo  1685   \n",
       "..     ...   ...   \n",
       "116    bpo  1967   \n",
       "20     bpo  1968   \n",
       "34     bpo  1969   \n",
       "87     bpo  1970   \n",
       "47     bpo  1974   \n",
       "\n",
       "                                                                              path  \\\n",
       "182  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1681.txt   \n",
       "56   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1682.txt   \n",
       "51   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1683.txt   \n",
       "60   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1684.txt   \n",
       "200  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1685.txt   \n",
       "..                                                                             ...   \n",
       "116  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1967.txt   \n",
       "20   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1968.txt   \n",
       "34   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1969.txt   \n",
       "87   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1970.txt   \n",
       "47   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1974.txt   \n",
       "\n",
       "        period  \n",
       "182             \n",
       "56              \n",
       "51              \n",
       "60              \n",
       "200             \n",
       "..         ...  \n",
       "116  1940-1980  \n",
       "20   1940-1980  \n",
       "34   1940-1980  \n",
       "87   1940-1980  \n",
       "47   1940-1980  \n",
       "\n",
       "[289 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfskip=get_skipgrams(calc_numlines=False)\n",
    "dfskip['period']=dfskip.year.apply(lambda y: periodize_bystep(y,40,1700))\n",
    "dfskip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfskip.groupby('period').num_words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfskipruns(dfskip,num_runs=10,incl_existing=False):\n",
    "    dfskipruns=pd.concat([\n",
    "        dfskip.assign(run=f'run_{str(i+1).zfill(2)}')\n",
    "        for i in range(num_runs)\n",
    "    ])\n",
    "    dfskipruns['opath']=dfskipruns.apply(lambda row: os.path.join(PATH_MODELS,row.corpus,row.period,row.run,'model.bin'),1)\n",
    "    dfskipruns['opath_exists']=dfskipruns.opath.apply(lambda x: os.path.exists(x))\n",
    "    if not incl_existing: dfskipruns=dfskipruns[dfskipruns.opath_exists==False]\n",
    "    return dfskipruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dfskipruns(dfskip,num_runs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkipgramsSampler2:\n",
    "    def __init__(self,fn,num_skips_wanted=100000,replace=True):\n",
    "        self.fn=fn\n",
    "        self.num_skips_wanted=num_skips_wanted\n",
    "        if os.path.exists(fn):\n",
    "            with xopen(fn) as f:\n",
    "                lines=[ln.strip().split() for ln in f if ln.strip()]\n",
    "        if not replace and len(lines)<num_skips_wanted:\n",
    "            num_skips_wanted=len(lines)\n",
    "        self.skips=random.choices(lines,k=num_skips_wanted)\n",
    "    def __iter__(self):\n",
    "        yield from self.skips\n",
    "    def __len__(self): return len(self.skips)\n",
    "\n",
    "\n",
    "\n",
    "class SkipgramsSampler:\n",
    "    def __init__(self, fn, num_skips_wanted=None, replace=False):\n",
    "            self.fn=fn\n",
    "            self.replace=replace\n",
    "            \n",
    "            if num_skips_wanted:                \n",
    "                self.num_skips=self.get_num_lines()\n",
    "                self.num_skips_wanted=nskip=num_skips_wanted if self.num_skips>num_skips_wanted else self.num_skips\n",
    "                self.line_nums_wanted = set(random.sample(list(range(nskip)), nskip))\n",
    "            else:\n",
    "                self.num_skips_wanted=None\n",
    "\n",
    "    def get_num_lines(self):\n",
    "            then=time.time()\n",
    "#               print('>> [SkipgramsSampler] counting lines in',self.fn)\n",
    "            with gzip.open(self.fn,'rb') if self.fn.endswith('.gz') else open(self.fn) as f:\n",
    "                    for i,line in enumerate(f):\n",
    "                            pass\n",
    "            num_lines=i+1\n",
    "            now=time.time()\n",
    "#               print('>> [SkipgramsSampler] finished counting lines in',self.fn,'in',int(now-then),'seconds. # lines =',num_lines,'and num skips wanted =',self.num_skips_wanted)\n",
    "            return num_lines\n",
    "\n",
    "    def __iter__(self):\n",
    "            i=0\n",
    "            with gzip.open(self.fn,'rb') if self.fn.endswith('.gz') else open(self.fn) as f:\n",
    "                    for i,line in enumerate(f):\n",
    "                            line = line.decode('utf-8') if self.fn.endswith('.gz') else line\n",
    "                            if not self.num_skips_wanted or i in self.line_nums_wanted:\n",
    "                                    yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramsSamplers:\n",
    "    def __init__(self,fns,nskip,sampler=SkipgramsSampler,**y):\n",
    "        self.skippers=[sampler(fn,nskip) for fn in fns]\n",
    "    def __iter__(self):\n",
    "        for skipper in self.skippers:\n",
    "            yield from skipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipfn=dfskip.iloc[0].path\n",
    "# iterr=SkipgramsSampler(skipfn,None)\n",
    "# l=[]\n",
    "# for x in tqdm(iterr): l+=[x]\n",
    "# print(random.choice(l))\n",
    "# print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_SKIP=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_save_model(dfskip,nskip=DEFAULT_NUM_SKIP,force=False,vector_size=100,window=10,min_count=5,epochs=10,workers=8,verbose=False):\n",
    "    row=dfskip.iloc[0]\n",
    "    odir=os.path.join(PATH_MODELS_NEW,row.corpus,row.period,row.run)\n",
    "    ofnfn=os.path.join(odir,'model.bin')\n",
    "    if force or not os.path.exists(ofnfn):\n",
    "        ensure_dir_exists(odir)\n",
    "        ss=SkipgramsSamplers(dfskip.path, nskip)\n",
    "        disable_gensim_logging() if not verbose else enable_gensim_logging()\n",
    "        model = Word2Vec(sentences=ss,vector_size=vector_size,window=window,min_count=min_count,epochs=epochs,workers=workers)\n",
    "        model.save(ofnfn)\n",
    "    return pd.DataFrame([{'fnfn':ofnfn}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnfn=gen_and_save_model(get_dfskipruns(dfskip).iloc[:1], force=True).fnfn.iloc[0]\n",
    "# load_model(fnfn).wv.most_similar('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res=pmap_groups(\n",
    "#     gen_and_save_model,\n",
    "#     dfskipruns.groupby(['period','run']),\n",
    "#     num_proc=4,\n",
    "#     kwargs=dict(force=True, nskip=NSKIP_PER_YR)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodize(1801,40,1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_models(\n",
    "        ybin=5,\n",
    "        ymin=1680,\n",
    "        ymax=1970,\n",
    "        num_runs=1,\n",
    "        force=False,\n",
    "        nskip_per_yr=NSKIP_PER_YR\n",
    "    ):\n",
    "    dfskip=get_skipgrams(calc_numlines=False).query(f'{ymin}<=year<{ymax}')\n",
    "    dfskip['period']=dfskip.year.apply(lambda y: periodize_bystep(y,ybin,ymin))\n",
    "    dfskip=dfskip[dfskip.period!='']\n",
    "    dfskipruns=get_dfskipruns(dfskip, num_runs=num_runs, incl_existing=force)\n",
    "    dfgrps=dfskipruns.groupby(['period','run'])\n",
    "    print(f'Generating {len(dfgrps)} new models over {dfskipruns.period.nunique()} periods and {dfskipruns.run.nunique()} runs')\n",
    "    return pmap_groups(\n",
    "        gen_and_save_model,\n",
    "        dfskipruns.groupby(['period','run']),\n",
    "        num_proc=5,\n",
    "        kwargs=dict(force=force, nskip=nskip_per_yr)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Koselleck] (10:24:05) Generating 5 new models over 5 periods and 1 runs (+34.5s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>year</th>\n",
       "      <th>path</th>\n",
       "      <th>period</th>\n",
       "      <th>run</th>\n",
       "      <th>opath</th>\n",
       "      <th>opath_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1700</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1700.txt</td>\n",
       "      <td>1700-1740</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1701</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1701.txt</td>\n",
       "      <td>1700-1740</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1702</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1702.txt</td>\n",
       "      <td>1700-1740</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1703</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1703.txt</td>\n",
       "      <td>1700-1740</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1704</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1704.txt</td>\n",
       "      <td>1700-1740</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1895</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1895.txt</td>\n",
       "      <td>1860-1900</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1896</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1896.txt</td>\n",
       "      <td>1860-1900</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1897</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1897.txt</td>\n",
       "      <td>1860-1900</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1898</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1898.txt</td>\n",
       "      <td>1860-1900</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>bpo</td>\n",
       "      <td>1899</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1899.txt</td>\n",
       "      <td>1860-1900</td>\n",
       "      <td>run_01</td>\n",
       "      <td>/home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus  year  \\\n",
       "157    bpo  1700   \n",
       "42     bpo  1701   \n",
       "25     bpo  1702   \n",
       "288    bpo  1703   \n",
       "69     bpo  1704   \n",
       "..     ...   ...   \n",
       "21     bpo  1895   \n",
       "108    bpo  1896   \n",
       "17     bpo  1897   \n",
       "285    bpo  1898   \n",
       "260    bpo  1899   \n",
       "\n",
       "                                                                              path  \\\n",
       "157  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1700.txt   \n",
       "42   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1701.txt   \n",
       "25   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1702.txt   \n",
       "288  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1703.txt   \n",
       "69   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1704.txt   \n",
       "..                                                                             ...   \n",
       "21   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1895.txt   \n",
       "108  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1896.txt   \n",
       "17   /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1897.txt   \n",
       "285  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1898.txt   \n",
       "260  /home/ryan/github/koselleck/data1/skipgrams/years/data.skipgrams.bpo.1899.txt   \n",
       "\n",
       "        period     run  \\\n",
       "157  1700-1740  run_01   \n",
       "42   1700-1740  run_01   \n",
       "25   1700-1740  run_01   \n",
       "288  1700-1740  run_01   \n",
       "69   1700-1740  run_01   \n",
       "..         ...     ...   \n",
       "21   1860-1900  run_01   \n",
       "108  1860-1900  run_01   \n",
       "17   1860-1900  run_01   \n",
       "285  1860-1900  run_01   \n",
       "260  1860-1900  run_01   \n",
       "\n",
       "                                                                       opath  \\\n",
       "157  /home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin   \n",
       "42   /home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin   \n",
       "25   /home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin   \n",
       "288  /home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin   \n",
       "69   /home/ryan/github/koselleck/data1/models/bpo/1700-1740/run_01/model.bin   \n",
       "..                                                                       ...   \n",
       "21   /home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin   \n",
       "108  /home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin   \n",
       "17   /home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin   \n",
       "285  /home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin   \n",
       "260  /home/ryan/github/koselleck/data1/models/bpo/1860-1900/run_01/model.bin   \n",
       "\n",
       "     opath_exists  \n",
       "157         False  \n",
       "42          False  \n",
       "25          False  \n",
       "288         False  \n",
       "69          False  \n",
       "..            ...  \n",
       "21          False  \n",
       "108         False  \n",
       "17          False  \n",
       "285         False  \n",
       "260         False  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_models(ybin=40,ymin=1700,ymax=1900,num_runs=1,nskip_per_yr=None)\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_models(num_runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def get_model_paths(model_dir=PATH_MODELS,model_fn='model.bin',vocab_fn='vocab.txt',period_len=None):\n",
    "    \"\"\"\n",
    "    Get all models' paths\n",
    "    \"\"\"\n",
    "    ld=[]\n",
    "    for root,dirs,fns in tqdm(os.walk(model_dir),desc='Scanning directory for models'):\n",
    "        if model_fn in fns:\n",
    "            corpus,period,run=root.split('/')[-3:]\n",
    "            if not 'run_' in run:\n",
    "                corpus,period=root.split('/')[-2:]\n",
    "                run=None\n",
    "            dx={\n",
    "                'corpus':corpus,\n",
    "                'period_start':int(period.split('-')[0]),\n",
    "                'period_end':int(period.split('-')[-1]),\n",
    "                'path':os.path.join(root,model_fn),\n",
    "                'path_vocab':os.path.join(root,vocab_fn)\n",
    "            }\n",
    "            if run is not None: dx['run']=run\n",
    "            if period_len and int(dx['period_end'])-int(dx['period_start'])!=period_len:\n",
    "                continue\n",
    "            ld.append(dx)\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 1900, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YMIN,YMAX,YEARBIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathdf_models(period_len=YEARBIN,ymin=YMIN,ymax=YMAX):\n",
    "    pathdf=pd.DataFrame(get_model_paths(PATH_MODELS_BPO, 'model.bin'))#.sort_values(['period_start','run'])\n",
    "    pathdf['period']=[f'{x}-{y}' for x,y in zip(pathdf.period_start, pathdf.period_end)]\n",
    "    pathdf['period_len']=pathdf.period_end - pathdf.period_start\n",
    "    pathdf['qstr']=[\n",
    "        f'vecs({period}_{run.split(\"_\")[-1]})'\n",
    "        for period,run in zip(pathdf.period, pathdf.run)\n",
    "    ]\n",
    "    if period_len: pathdf=pathdf[pathdf.period_len==period_len]\n",
    "    if ymin: pathdf=pathdf[pathdf.period_start>=ymin]\n",
    "    if ymax: pathdf=pathdf[pathdf.period_end<=ymax]\n",
    "    return pathdf[~pathdf.period.isnull()].sort_values('period_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pathdf_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_models(ymin=YMIN,ymax=YMAX,ybin=YEARBIN,num_runs=10):\n",
    "    if os.path.exists(FN_DEFAULT_MODEL_PATHS):\n",
    "        odf=read_df(FN_DEFAULT_MODEL_PATHS)\n",
    "    else:\n",
    "        odf=get_pathdf_models(period_len=ybin)\n",
    "        odf.to_pickle(FN_DEFAULT_MODEL_PATHS)\n",
    "    return odf.query(f'{ymin}<=period_start & period_end<={ymax} & run<=\"run_{num_runs:02}\"')\n",
    "    \n",
    "\n",
    "def get_default_periods(**y):\n",
    "    return sorted(list(set(get_default_models(**y).period)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_periods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_periods_runs(period_or_periods=None,run_or_runs=None,num_runs=10):\n",
    "    periods=period_or_periods\n",
    "    if periods is None: periods=get_default_periods()\n",
    "    if type(periods)==str: periods=tokenize_fast(periods)\n",
    "    periods=set(periods)\n",
    "    runs=run_or_runs    \n",
    "    if runs is None: runs=list(range(1,num_runs+1))\n",
    "    if type(runs)==int: runs=[runs]\n",
    "    if type(runs)==str: runs=[int(runs)]\n",
    "    runs=set(runs)\n",
    "    return periods,runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_periods_runs('1770-1775,1780-1785')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_periods(ymin=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(path_model,path_vocab=None,min_count=None,cache_bin=True,cache=False):\n",
    "    global MODEL_CACHE\n",
    "    \n",
    "    if cache and path_model in MODEL_CACHE: return MODEL_CACHE[path_model]\n",
    "#     print('Loading',path_model)\n",
    "    model=do_load_model(path_model,path_vocab=path_vocab,min_count=min_count,cache_bin=cache_bin)\n",
    "    return model\n",
    "    \n",
    "def do_load_model(path_model,path_vocab=None,min_count=None,cache_bin=True):\n",
    "#     print('>> loading',path_model)\n",
    "    path_model_bin=path_model.split('.txt')[0]+'.bin' if not path_model.endswith('.bin') else path_model\n",
    "    if os.path.exists(path_model_bin):\n",
    "        model=gensim.models.KeyedVectors.load(path_model_bin,mmap='r')\n",
    "    elif os.path.exists(path_model):\n",
    "        if not path_vocab: path_vocab=os.path.join(os.path.dirname(path_model,'vocab.txt'))\n",
    "        if os.path.exists(path_vocab):\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path_model,path_vocab)\n",
    "            if min_count: filter_model(model,min_count=min_count)\n",
    "        else:\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path_model)\n",
    "        if cache_bin:\n",
    "            model.save(path_model_bin)\n",
    "    else:\n",
    "        print('!!??',path_model)\n",
    "        stop\n",
    "        return None\n",
    "#     print(path_model, len(model.wv.key_to_index))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = get_default_models().sample(n=1).iloc[0]\n",
    "# load_model_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=load_model('/home/ryan/github/koselleck/data1/models/bpo/1805-1810/run_25/model.bin')\n",
    "# m.wv.most_similar('virtue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=load_model('/home/ryan/github/koselleck/data1/models/bpo/1945-1950/run_07/model.bin')\n",
    "# m.wv.most_similar(['king','woman'],['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(dfmodels,gby=['period','run']):\n",
    "    o=[]\n",
    "    dfgrp=dfmodels.groupby(gby)\n",
    "    for period,dfg in tqdm(sorted(dfgrp)):#, total=len(dfgrp)):\n",
    "        path=dfg.iloc[-1].path\n",
    "        m=load_model(path)\n",
    "        try:\n",
    "            testvec=m.wv.most_similar(['king','woman'],['man'],topn=25)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        testvec_wl=[x for x,y in testvec]\n",
    "        has_queen='queen' in set(testvec_wl)\n",
    "        odx={\n",
    "            **dict(zip(gby,period)),\n",
    "            'has_queen':has_queen,\n",
    "            'rank_queen':testvec_wl.index('queen') if has_queen else np.nan,\n",
    "            'neighborhood':', '.join(testvec_wl),\n",
    "        }\n",
    "        o+=[odx]\n",
    "#         break\n",
    "    return pd.DataFrame(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmodels = get_pathdf_models().query('period_len==5')\n",
    "# dftests  = test_models(dfmodels)\n",
    "# dftests.to_csv('../../data/data.model.tests.csv')\n",
    "# dftests.query('has_queen==True').groupby('period').size()\n",
    "# dftests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_veclib_word_data_path(word):\n",
    "    ofn=os.path.join(PATH_DB,'cdists',f'data.cdists.{word}.pkl.gz')\n",
    "    odir=os.path.dirname(ofn)\n",
    "    if not os.path.exists(odir): os.makedirs(odir)\n",
    "    return ofn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ryan/github/koselleck/db/cdists/data.cdists.virtue.pkl.gz'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_new_veclib_word_data_path('virtue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_veclib_word_data(word,progress=True,cache=True,cache_only=False,force=False,remove_old=True,\n",
    "                        periods=None):\n",
    "    if progress: print(f'Loading cdist data for \"{word}\"')\n",
    "    odf=pd.DataFrame()\n",
    "    fnfn=get_new_veclib_word_data_path(word)\n",
    "    oldfnfn=get_old_veclib_word_data_path(word)\n",
    "    if cache and not force and os.path.exists(fnfn):\n",
    "        try:\n",
    "            odf=read_df(fnfn)\n",
    "            if progress: print(f'Finished loading cdist data from pkl for \"{word}\"')\n",
    "        except Exception as e:\n",
    "            print('!!',e)\n",
    "    if not len(odf):\n",
    "        if not os.path.exists(oldfnfn):\n",
    "            if progress: print(f'No file found at {oldfnfn}')\n",
    "        else:\n",
    "            with get_veclib_word(word) as vl:\n",
    "                dfdist=pd.DataFrame(dict(vl.items())).T.rename_axis('period_run_')\n",
    "                dfdist['period_'],dfdist['run_']=zip(*[x.split('_') for x in dfdist.index])\n",
    "                dfdist['run_']=dfdist['run_'].apply(int)\n",
    "                odf=dfdist.reset_index().drop('period_run_',1).set_index(['period_','run_'])\n",
    "                if cache:\n",
    "                    if progress: print(f'Saving dfdist to \"{fnfn}\"')\n",
    "                    odf.to_pickle(fnfn)\n",
    "                    if remove_old and os.path.exists(oldfnfn):\n",
    "                        if progress: print(f'Removing old data from \"{oldfnfn}\"')\n",
    "                        os.remove(oldfnfn)\n",
    "                if progress: print(f'Finished loading cdist data from sqlite for \"{word}\"')\n",
    "    if not len(odf): return odf\n",
    "    odf['word_']=word\n",
    "    odf=odf.reset_index()\n",
    "    \n",
    "    if periods is None: periods=set(get_default_periods())\n",
    "    odf=odf[odf.period_.isin(periods)]\n",
    "    odf=odf.set_index(['word_','period_','run_']).rename_axis(['word','period','run'])\n",
    "    return odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfdist=get_veclib_word_data('histories',force=False,remove_old=False,cache_only=False)\n",
    "# dfdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_in_sqlite_data():\n",
    "    fns=os.listdir(os.path.join(PATH_DB,'wvecs'))\n",
    "    words=[fn.split('.sqlite')[0].split('.')[-1] for fn in fns]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=get_all_words_in_sqlite_data()\n",
    "# len(words),random.sample(words,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_veclib_word_data_(objd): return get_veclib_word_data(**objd)\n",
    "def reformat_all_sqlite_data(words=None,lim=None,num_proc=1,remove_old=True):\n",
    "    words=get_all_words_in_sqlite_data()[:lim] if words is None else list(words)[:lim]\n",
    "    return pmap(\n",
    "        _get_veclib_word_data_,\n",
    "        [dict(word=word,progress=False,cache=True,force=True,remove_old=remove_old,\n",
    "              cache_only=True) for word in words],\n",
    "        num_proc=num_proc,\n",
    "        desc='Reformatting old sqlite data into pkl files',\n",
    "        use_threads=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=get_valid_words()\n",
    "# random.shuffle(words)\n",
    "# res=reformat_all_sqlite_data(words,lim=None,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def measure_ambiguity(model,words=None,topn=10):\n",
    "    dfdist=to_dist(model,words=words)\n",
    "    g=to_semnet_from_dist(dfdist,topn=topn)\n",
    "    s=pd.Series(nx.clustering(g)).sort_values()\n",
    "    amb=1-s\n",
    "    return amb\n",
    "\n",
    "def to_dist(m,words=None,z=1,norm=1,maxwords=10000):\n",
    "    if words is None:\n",
    "        words=[m.wv.index_to_key[i] for i in range(maxwords)]\n",
    "    else:\n",
    "        words=set(words) & set(m.wv.key_to_index.keys())\n",
    "    words=list(words)\n",
    "    vecs = np.array([m.wv[w] for w in words], dtype=np.float64)\n",
    "    omatrix = fastdist.cosine_pairwise_distance(vecs, return_matrix=True)\n",
    "    odf = round(pd.DataFrame(omatrix, index=words, columns=words),6)\n",
    "    maxv=odf.max().max()\n",
    "    odf=odf.replace({maxv:np.nan})\n",
    "    if norm: odf=maxv - odf\n",
    "    if z: odf=(odf - odf.mean().mean())/odf.std().std()\n",
    "    return odf\n",
    "\n",
    "def to_semnet_from_dist(dfdist,cutoff=3,topn=10):\n",
    "    g=nx.Graph()\n",
    "    for word1 in dfdist.columns:\n",
    "        for word2,val in dfdist[word1].sort_values(ascending=True).iloc[:topn].items():\n",
    "            g.add_edge(word1,word2,weight=val*-1)\n",
    "    return g\n",
    "\n",
    "def measure_ambiguity(model,words=None,topn=10,z=False):\n",
    "    dfdist=to_dist(model,words=words)\n",
    "    g=to_semnet_from_dist(dfdist,topn=topn)\n",
    "    s=pd.Series(nx.clustering(g)).sort_values()\n",
    "    amb=1-s\n",
    "    if z: amb=(amb - amb.mean())/amb.std()\n",
    "    return amb\n",
    "\n",
    "def get_any_model(dfpath=None):\n",
    "    if dfpath is None: dfpath=get_pathdf_models_bydecade()\n",
    "    row=dfpath.sample(n=1).iloc[0]\n",
    "    return load_model_row(row)\n",
    "\n",
    "def measure_freq(model,words=None,tf=True,z=False):\n",
    "    mwords=set(model.wv.key_to_index.keys())\n",
    "    words=mwords if not words else mwords&set(words)\n",
    "    vocabd=dict(\n",
    "        (\n",
    "            w,\n",
    "            model.wv.get_vecattr(w,'count')\n",
    "        )\n",
    "        for w in words\n",
    "    )\n",
    "    svocab=pd.Series(vocabd)\n",
    "    if tf: svocab=svocab / svocab.sum()\n",
    "    if z: svocab=(svocab - svocab.mean()) / svocab.std()\n",
    "    return svocab\n",
    "\n",
    "INFLECTER=None\n",
    "def measure_singularism(m,words=None,z=True):\n",
    "    global INFLECTER\n",
    "    if INFLECTER is None:\n",
    "        import inflect\n",
    "        INFLECTER=inflect.engine() \n",
    "    p=INFLECTER\n",
    "\n",
    "    if not words: words=get_valid_words()\n",
    "    words=list(set(words) & set(m.wv.key_to_index.keys()))\n",
    "    words_plurals = pmap(p.plural_noun, words, num_proc=1, progress=False)\n",
    "    s=measure_freq(m,words=set(words+words_plurals),z=z)\n",
    "    sd=dict(s)\n",
    "    odf=pd.DataFrame([\n",
    "        {'word':ws, 'word_pural':wp, 'freq_sing':sd.get(ws), 'freq_plural':sd.get(wp)}\n",
    "        for ws,wp in zip(words,words_plurals)\n",
    "        if ws!=wp\n",
    "    ])\n",
    "    odf['freq_diff']=odf['freq_sing']-odf['freq_plural']\n",
    "    odf['rank_sing']=odf['freq_sing'].rank(ascending=True)\n",
    "    odf['rank_plural']=odf['freq_plural'].rank(ascending=True)\n",
    "    odf['rank_diff']=odf['rank_sing']-odf['rank_plural']\n",
    "    if z:\n",
    "        for x in odf.select_dtypes('number').columns:\n",
    "            odf[x]=(odf[x] - odf[x].mean())/odf[x].std()\n",
    "    return odf.set_index('word').sort_values('rank_diff').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None, combine_models=False):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    if combine_models:\n",
    "        o=union_align_gensim(base_embed,other_embed)\n",
    "    else:\n",
    "        o=other_embed\n",
    "    return o\n",
    "    \n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "    return (m1,m2)\n",
    "\n",
    "\n",
    "def union_align_gensim(m1, m2, words=None, suffix1='_m1',suffix2='_m2'):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    new_arr_both=[]\n",
    "    common_vocab_both=[w+suffix1 for w in common_vocab] + [w+suffix2 for w in common_vocab]\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = [old_arr[index] for index in indices]\n",
    "        new_arr_both+=new_arr\n",
    "    \n",
    "#     odf=pd.DataFrame(new_arr_both,index=common_vocab_both)\n",
    "#     return odf\n",
    "    \n",
    "    m.wv.vectors = np.array(new_arr_both)\n",
    "\n",
    "    # Replace old vocab dictionary with new one (with common vocab)\n",
    "    # and old index2word with new one\n",
    "    new_key_to_index = {}\n",
    "    new_index_to_key = []\n",
    "    for new_index, key in enumerate(common_vocab_both):\n",
    "        new_key_to_index[key] = new_index\n",
    "        new_index_to_key.append(key)\n",
    "    m.wv.key_to_index = new_key_to_index\n",
    "    m.wv.index_to_key = new_index_to_key\n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
